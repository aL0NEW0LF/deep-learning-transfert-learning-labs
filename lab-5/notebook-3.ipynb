{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import gaussian_kde\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16 \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(root = './assets/flowers102', split = 'train', download = True, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.Flowers102(root = './assets/flowers102', split = 'test', download = True, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = False, num_workers=2)\n",
    "\n",
    "valset = torchvision.datasets.Flowers102(root = './assets/flowers102', split = 'val', download = True, transform = transform)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = batch_size, shuffle = True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_FROM_SCRATCH(nn.Module):\n",
    "    def __init__(self, kernel_size = 3):\n",
    "        super(CNN_FROM_SCRATCH, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size = kernel_size, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size = kernel_size, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2, padding=0, dilation=1, ceil_mode=False)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128*64*64, 1024, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5, inplace = False),\n",
    "            nn.Linear(512, 102, bias = True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 128*64*64)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNN_FROM_SCRATCH()\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# validation function\n",
    "def validate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    for int, data in enumerate(val_dataloader):\n",
    "        data, target = data[0].to(device), data[1].to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        val_running_loss += loss.item()\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        val_running_correct += (preds == target).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss/len(val_dataloader.dataset)\n",
    "    val_accuracy = 100. * val_running_correct/len(val_dataloader.dataset)\n",
    "    \n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# training function\n",
    "def fit(model, train_dataloader):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        data, target = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_running_loss += loss.item()\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "        train_running_correct += (preds == target).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
    "    \n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    for i, data in enumerate(data):\n",
    "        data = data[0].to(device)\n",
    "        output = model(data)\n",
    "        _, preds = torch.max(output.data, 1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss , train_accuracy = [], []\n",
    "val_loss , val_accuracy = [], []\n",
    "start = time.time()\n",
    "for epoch in range(5):\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model, valloader)\n",
    "    print(f'Epoch {epoch+1} | Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f} | Validation Loss: {val_epoch_loss:.4f}, Validation Acc: {val_epoch_accuracy:.2f}')\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "end = time.time()\n",
    "\n",
    "print((end-start)/60, 'minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
